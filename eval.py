import os
import glob
import time
from dotenv import load_dotenv
from openai import OpenAI, AsyncOpenAI
import asyncio
import math
from concurrent.futures import ThreadPoolExecutor
import random

# Load environment variables from .env file
load_dotenv()

# Retrieve environment variables
SOLVER_LLM = os.getenv("SOLVER_LLM")
CHECKER_LLM = os.getenv("CHECKER_LLM")
SOLVER_API_BASE = os.getenv("SOLVER_API_BASE")
SOLVER_API_KEY = os.getenv("SOLVER_API_KEY")
CHECKER_API_BASE = os.getenv("CHECKER_API_BASE")
CHECKER_API_KEY = os.getenv("CHECKER_API_KEY")
SYSTEM_SUPPORTED = os.getenv("SYSTEM_SUPPORTED") == "True"

# Configure OpenAI API
solver_client = OpenAI(api_key=SOLVER_API_KEY, base_url=SOLVER_API_BASE)
checker_client = OpenAI(api_key=CHECKER_API_KEY, base_url=CHECKER_API_BASE)

def solve_question(question_text, model):
    """
    Uses the specified LLM to generate an answer to the given question.
    Retries up to 3 times if the response fails.

    Args:
        question_text: The text of the question.
        model: The name of the LLM model to use.

    Returns:
        The generated answer, or None if an error occurred after retries.
    """
    retries = 0
    max_retries = 8
    while retries < max_retries:
        try:
            if SYSTEM_SUPPORTED:
                response = solver_client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that answers questions accurately. At the end of your response, put your answer like this: ANSWER: (This is where you put your response.)"},
                        {"role": "user", "content": question_text}
                    ],
                    temperature=0,
                )
            else: 
                response = solver_client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "user", "content": "You are a helpful assistant that answers questions accurately. At the end of your response, put your answer like this: ANSWER: (This is where you put your response.) \nQUESTION: " + question_text}
                    ],
                )
            return response.choices[0].message.content.strip(), response.choices[0].message.content.strip().split("ANSWER")[-1]
        except Exception as e:
            print(f"Error solving question with {model}: {e}")
            retries += 1
            time.sleep(45)  # Wait for a second before retrying
    print(f"Failed to solve question after {max_retries} attempts.")
    return None, None

def check_answer(question, ground_truth_answer, solver_answer, model):
    """
    Uses the specified LLM to check if the solver's answer is correct.
    Retries up to 3 times if the response fails.

    Args:
        question_text: The text of the question.
        solver_answer: The answer generated by the solver LLM.
        model: The name of the LLM model to use for checking.

    Returns:
        True if the answer is deemed correct, False otherwise.
    """
    retries = 0
    max_retries = 8
    while retries < max_retries:
        try:
            prompt = f"""Question: {question}

            GROUND TRUTH ANSWER: {ground_truth_answer}

            Solver's Answer: {solver_answer}

            Look into the response given by solver's answer and compare it with the ground truth.
            Are the results the same?
            DON'T CHECK WHETHER SOLVER'S REASONING IS CORRECT OR NOT, JUST CHECK FOR THE END RESULT.
            If the answer cannot be numerically evaluated, check if the two answers denote the same thing. Just output yes OR no in this case.
            If the answer can be numerically evaluated, output an array where both ground truth answer and solver answer is present as both elements, ready to be parsed by Python's eval function.
            For example, if ground truth is: 2*pi/3 and the answer given is: pi*2/3, output an array that can be parsed. DON'T WRITE THE WHOLE SCRIPT, JUST OUTPUT THE ARRAY.
            Also assume math library is imported.
            So, in here your output would be only: [2*math.pi/3, math.pi*2/3]
            ONLY DO THIS WHEN THE ANSWER IS A NUMBER.
            """
            response = checker_client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant that evaluates whether two answers are the same."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,  # Adjust temperature as needed
                max_tokens=200,  # Adjust max_tokens as needed
            )
            checker_response = response.choices[0].message.content.strip().strip("`").strip("'").strip('"').strip().lower()
            print(checker_response)
            if checker_response in ["yes", "no"]:
                return checker_response == "yes"
            else:
                checker_response = checker_response.strip("`").strip("'").strip('"').strip("json").strip("python").strip()
                arr = eval(checker_response)
                ans1, ans2 = arr
                if type(ans1) is str:
                    ans1 = eval(ans1)
                if type(ans2) is str:
                    ans2 = eval(ans2)
                return are_close(ans1, ans2)

        except Exception as e:
            print(f"Error checking answer with {model}: {e}")
            retries += 1
            time.sleep(45 + random.randint(10, 30))
    print(f"Failed to check answer after {max_retries} attempts.")
    return False
    
def are_close(a, b, tolerance=1e-12):
    """Checks if two numbers are approximately equal within a tolerance.

    Args:
        a: The first number.
        b: The second number.
        tolerance: The acceptable difference between the numbers.

    Returns:
        True if the numbers are close, False otherwise.
    """
    return abs(a - b) <= tolerance

async def evaluate_question(question_file):
    """
    Evaluates a single question.

    Args:
        question_file: The path to the question file.

    Returns:
        A dictionary containing the evaluation result for the question.
    """
    with open(question_file, "r", encoding="utf-8") as f:
        question_text = f.read()

    print(f"Solving question: {os.path.basename(question_file)}")
    question = question_text.split("ANSWER =")[0]
    ground_truth = question_text.split("ANSWER =")[1]

    if ground_truth.strip() == "???":
        print(f"Skipping question due to no answer: {os.path.basename(question_file)}")
        return {
            "question_file": os.path.basename(question_file),
            "question": question_text,
            "solver_answer": None,
            "is_correct": None
        }

    with ThreadPoolExecutor(max_workers=1) as executor:
        solver_response, solver_answer = await asyncio.get_running_loop().run_in_executor(
            executor, solve_question, question, SOLVER_LLM
        )

    if solver_answer:
        print(f"Solver's answer: {solver_answer}")
        with ThreadPoolExecutor(max_workers=1) as executor:
            is_correct = await asyncio.get_running_loop().run_in_executor(
                executor, check_answer, question, ground_truth, solver_answer, CHECKER_LLM
            )

        if is_correct:
            print("Answer is correct.")
        else:
            print("Answer is incorrect.")

        return {
            "question_file": os.path.basename(question_file),
            "question": question_text,
            "solver_answer": solver_response,
            "is_correct": is_correct
        }
    else:
        return {
            "question_file": os.path.basename(question_file),
            "question": question_text,
            "solver_answer": None,
            "is_correct": None
        }

async def evaluate_benchmark(questions_folder):
    """
    Evaluates the LLM benchmark by solving questions and checking answers.

    Args:
        questions_folder: The path to the folder containing the question files.

    Returns:
        A dictionary containing the evaluation results.
    """
    question_files = glob.glob(os.path.join(questions_folder, "*.txt"))
    results = {
        "total_questions": len(question_files),
        "correct_answers": 0,
        "incorrect_answers": 0,
        "solver_errors": 0,
        "checker_errors": 0,
        "details": []
    }

    tasks = [evaluate_question(question_file) for question_file in question_files]
    question_results = await asyncio.gather(*tasks)

    for result in question_results:
        results["details"].append(result)
        if result["solver_answer"] is None:
            results["solver_errors"] += 1
        elif result["is_correct"] is None:
            results["checker_errors"] +=1 # should not happen, but just in case
        elif result["is_correct"]:
            results["correct_answers"] += 1
        else:
            results["incorrect_answers"] += 1

    return results

if __name__ == "__main__":
    questions_folder = "questions"  # Replace with the actual path to your questions folder
    async def main():
        evaluation_results = await evaluate_benchmark(questions_folder)

        print("\nEvaluation Results:")
        print(f"Total Questions: {evaluation_results['total_questions']}")
        print(f"Correct Answers: {evaluation_results['correct_answers']}")
        print(f"Incorrect Answers: {evaluation_results['incorrect_answers']}")
        print(f"Solver Errors: {evaluation_results['solver_errors']}")
        print(f"Checker Errors: {evaluation_results['checker_errors']}")
        print(f"Accuracy: {evaluation_results['correct_answers'] / evaluation_results['total_questions'] * 100:.2f}%")

        # You can further process or save the detailed results as needed
        # For example, save to a JSON file:
        import json
        with open(f"evaluation_results_{SOLVER_LLM.replace("/", "_")}.json", "w") as f:
            json.dump(evaluation_results, f, indent=4)
    
    asyncio.run(main())